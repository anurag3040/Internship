{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ad626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb62e710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\AnuragMishra\\Downloads\\chromedriver_win32.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de281e02",
   "metadata": {},
   "source": [
    "# 1. Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304a5bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the naukari page on selenium automated chrome browser\n",
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9f09f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering the Job position & Location as required\n",
    "designation=driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation.send_keys('Data Analyst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b74a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "location=driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/div[1]/div/input\")\n",
    "location.send_keys('Bangalore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8596389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04949bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286e7cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping job title from naukari page\n",
    "title1 = driver.find_elements(By.XPATH, '//a[@class=\"title ellipsis\"]')\n",
    "for i in title1[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "\n",
    "#scrapping job location from naukari page\n",
    "location1 = driver.find_elements(By.XPATH, '//span[@class=\"ellipsis fleft locWdth\"]')\n",
    "for i in location1[0:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "\n",
    "#scrapping company name from naukari page\n",
    "company1 = driver.find_elements(By.XPATH, '//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company1[0:10]:\n",
    "    company=i.text\n",
    "    company_name.append(company)\n",
    "\n",
    "#scrapping experience required from naukari page\n",
    "experience1 = driver.find_elements(By.XPATH, '//span[@class=\"ellipsis fleft expwdth\"]')\n",
    "for i in experience1[0:10]:\n",
    "    exp=i.text\n",
    "    experience_required.append(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45dcaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(job_title),len(job_location),len(company_name),len(experience_required))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae83e952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({'Title':job_title,'Location':job_location,'Company':company_name,'Experience':experience_required})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcfcfb9",
   "metadata": {},
   "source": [
    "# 2. Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1d8590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\AnuragMishra\\Downloads\\chromedriver_win32.exe\")\n",
    "# opening the naukari page on selenium automated chrome browbScrapping\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0298b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the naukari page on selenium automated chrome browser\n",
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25024122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering the Job position & Location as required\n",
    "position=driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "position.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011f6144",
   "metadata": {},
   "outputs": [],
   "source": [
    "location2=driver.find_element(By.XPATH,\"/html/body/div[1]/div[7]/div/div/div[5]/div/div/div/div[1]/div/input\")\n",
    "location2.send_keys('Bangalore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525dae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1590843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title1=[]\n",
    "job_location1=[]\n",
    "company_name1=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfcf152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping job title from naukari page\n",
    "title2 = driver.find_elements(By.XPATH, '//a[@class=\"title ellipsis\"]')\n",
    "for i in title2[0:10]:\n",
    "    title=i.text\n",
    "    job_title1.append(title)\n",
    "\n",
    "#scrapping job location from naukari page\n",
    "location3 = driver.find_elements(By.XPATH, '//span[@class=\"ellipsis fleft locWdth\"]')\n",
    "for i in location3[0:10]:\n",
    "    location2=i.text\n",
    "    job_location1.append(location2)\n",
    "\n",
    "#scrapping company name from naukari page\n",
    "company2 = driver.find_elements(By.XPATH, '//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company2[0:10]:\n",
    "    company=i.text\n",
    "    company_name1.append(company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00595e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(job_title1),len(job_location1),len(company_name1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f0d2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1=pd.DataFrame({'Title':job_title1,'Location':job_location1,'Company':company_name1})\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9aef91",
   "metadata": {},
   "source": [
    "# 3. In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb84839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\AnuragMishra\\Downloads\\chromedriver_win32.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34922fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the naukari page on selenium automated chrome browser\n",
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e57302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering the Job position & Location as required\n",
    "position=driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "position.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9164cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16df833",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_delhi=driver.find_element(By.XPATH,\"/html/body/div[1]/div[4]/div/div/section[1]/div[2]/div[5]/div[2]/div[2]/label/i\")\n",
    "location_delhi.click()\n",
    "# filter1=driver.find_element(By.XPATH,'//i[@class=\"fleft naukicon naukicon-checkbox\"]')\n",
    "# filter1.click()\n",
    "# location_search_box = driver.find_element(By.CLASS_NAME, 'filterOptns')\n",
    "# location_search_box.clear()  # clear any existing text in the search box\n",
    "# location_search_box.send_keys('Delhi/NCR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8823164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary=driver.find_element(By.XPATH,\"/html/body/div[1]/div[4]/div/div/section[1]/div[2]/div[6]/div[2]/div[2]/label/i\")\n",
    "salary.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d766eda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title1=[]\n",
    "job_location1=[]\n",
    "company_name1=[]\n",
    "experience_required1=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c4fcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping job title from naukari page\n",
    "title2 = driver.find_elements(By.XPATH, '//a[@class=\"title ellipsis\"]')\n",
    "for i in title2[0:10]:\n",
    "    titles=i.text\n",
    "    job_title1.append(titles)\n",
    "\n",
    "#scrapping job location from naukari page\n",
    "location2 = driver.find_elements(By.XPATH, '//span[@class=\"ellipsis fleft locWdth\"]')\n",
    "for i in location2[0:10]:\n",
    "    locations=i.text\n",
    "    job_location1.append(locations)\n",
    "\n",
    "#scrapping company name from naukari page\n",
    "company2 = driver.find_elements(By.XPATH, '//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company2[0:10]:\n",
    "    companys=i.text\n",
    "    company_name1.append(companys)\n",
    "\n",
    "#scrapping experience required from naukari page\n",
    "experience2 = driver.find_elements(By.XPATH, '//span[@class=\"ellipsis fleft expwdth\"]')\n",
    "for i in experience2[0:10]:\n",
    "    exp1=i.text\n",
    "    experience_required1.append(exp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9806f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.DataFrame({'Title':job_title1,'Location':job_location1,'Company':company_name1,'Experience':experience_required1})\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fc6092",
   "metadata": {},
   "source": [
    "# 4. Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and\n",
    "click the search icon\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the\n",
    "required data as usual.\n",
    "4. After scraping data from the first page, go to the “Next” Button at the bottom other page , then\n",
    "click on it.\n",
    "5. Now scrape data from this page as usual\n",
    "6. Repeat this until you get data for 100 sunglasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9db8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\AnuragMishra\\Downloads\\chromedriver_win32.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ef5839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the flipkart page on selenium automated chrome browser\n",
    "driver.get(\"https://www.flipkart.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40adafc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering the Job position & Location as required\n",
    "products=driver.find_element(By.CLASS_NAME,\"_3704LK\")\n",
    "products.send_keys('sunglasses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab283d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.CLASS_NAME,\"_34RNph\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71c961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand = []\n",
    "description = []\n",
    "price = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925e685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping brand from flipkart page\n",
    "start = 0\n",
    "end = 3\n",
    "brand1 = driver.find_elements(By.CLASS_NAME, '_2WkVRV')\n",
    "for i in brand1:\n",
    "    brands=i.text\n",
    "    brand.append(brands)\n",
    "brand_name = brand[0:100]\n",
    "\n",
    "#scrapping description from flipkart page\n",
    "start = 0\n",
    "end = 3\n",
    "description1 = driver.find_elements(By.CLASS_NAME, 'IRpwTa')\n",
    "for i in description1:\n",
    "    descriptions=i.text\n",
    "    description.append(descriptions)\n",
    "description2 = description[0:100]\n",
    "\n",
    "#scrapping price from flipkart page\n",
    "start = 0\n",
    "end = 3\n",
    "price1 = driver.find_elements(By.CLASS_NAME, '_30jeq3')\n",
    "for i in price1:\n",
    "    prices=i.text\n",
    "    price.append(prices)\n",
    "price2 = price[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c84c6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(brand_name), len(description2), len(price2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d0996d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=pd.DataFrame({'Brand':brand_name,'Description':description2,'Price':price2})\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc1d7d7",
   "metadata": {},
   "source": [
    "# 5. Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb/productreviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\n",
    "As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd26a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\AnuragMishra\\Downloads\\chromedriver_win32.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dedb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the flipkart given page on selenium automated chrome browser\n",
    "# given link in the ques is not available or product removed from website\n",
    "driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb/p/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART&q=iphone+11&store=tyy%2F4io&srno=s_1_4&otracker=search&otracker1=search&fm=organic&iid=77b69675-8ec5-4d27-8a2b-0aca89ef221e.MOBFWQ6BXGJCEYNY.SEARCH&ppt=hp&ppn=homepage&ssid=ktv3hdy93k0000001683543269877&qH=f6cdfdaa9f3c23f3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc9e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[10]/div[7]/div/a/div/span\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efda6092",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating = []\n",
    "summary = []\n",
    "review = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f146c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping brand from flipkart page\n",
    "start = 0\n",
    "end = 100\n",
    "for page in range(start,end):\n",
    "    rating1 = driver.find_elements(By.XPATH, '//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "    for i in rating1:\n",
    "        ratings=i.text\n",
    "        rating.append(ratings)\n",
    "    next_button = driver.find_element(By.XPATH, '//a[@class=\"_1LKTO3\"]') #to scrap data from next pages\n",
    "    next_button.click()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2959cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping brand from flipkart page\n",
    "start = 0\n",
    "end = 10\n",
    "for page in range(start,end):\n",
    "    summary1 = driver.find_elements(By.XPATH, '//p[@class=\"_2-N8zT\"]')\n",
    "    for i in summary1:\n",
    "        summarys=i.text\n",
    "        summary.append(summarys)\n",
    "    next_button = driver.find_element(By.XPATH, '//span') #to scrap data from next pages\n",
    "    next_button.click()\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daa155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping brand from flipkart page\n",
    "start = 0\n",
    "end = 10\n",
    "for page in range(start,end):\n",
    "    review1 = driver.find_elements(By.XPATH, '//div[@class=\"t-ZTKy\"]')\n",
    "    for i in review1:\n",
    "        reviews=i.text\n",
    "        review.append(reviews)\n",
    "    next_button = driver.find_element(By.XPATH, '//span') #to scrap data from next pages\n",
    "    next_button.click()\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2462ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(review),len(summary),len(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0898c676",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=pd.DataFrame({'Rating':rating,'Summary':summary,'Full Review':review})\n",
    "df4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8b4b7c",
   "metadata": {},
   "source": [
    "# 6. Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field. You have to scrape 3 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "As shown in the below image, you have to scrape the above attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c95f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\AnuragMishra\\Downloads\\chromedriver_win32.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d86f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the flipkart given page on selenium automated chrome browser\n",
    "driver.get(\"https://www.flipkart.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7502696",
   "metadata": {},
   "outputs": [],
   "source": [
    "product=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\")\n",
    "product.send_keys('sneakers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54318e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.CLASS_NAME,\"L0Z3Pu\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9728d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sneaker_brand = []\n",
    "sneaker_description = []\n",
    "brand11 = []\n",
    "description11 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8ffb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping brand from flipkart page\n",
    "start = 0\n",
    "end = 3\n",
    "sbrand = driver.find_elements(By.CLASS_NAME, '_2WkVRV')\n",
    "for i in sbrand:\n",
    "    sbrand1=i.text\n",
    "    sneaker_brand.append(sbrand1)\n",
    "    \n",
    "brand11 = sneaker_brand[0:100]\n",
    "\n",
    "#scrapping description from flipkart page\n",
    "start = 0\n",
    "end = 3\n",
    "sdescription = driver.find_elements(By.CLASS_NAME, 'IRpwTa')\n",
    "for i in sdescription:\n",
    "    sdescription1=i.text\n",
    "    sneaker_description.append(sdescription1)\n",
    "description11 = sneaker_description[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a06eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(brand11), len(description11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f735091",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5=pd.DataFrame({'Brand':brand11,'Description':description11})\n",
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98f78ad",
   "metadata": {},
   "source": [
    "# 9. Write a python program to display list of respected former Prime Ministers of India(i.e. Name, Born-Dead, Term of office, Remarks) from https://www.jagranjosh.com/.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.jagranjosh.com/\n",
    "2. Then You have to click on the GK option\n",
    "3. Then click on the List of all Prime Ministers of India\n",
    "4. Then scrap the mentioned data and make the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "2c118117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\AnuragMishra\\Downloads\\chromedriver_win32.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "f0391bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the motor1 page on selenium automated chrome browser\n",
    "driver.get(\"https://www.jagranjosh.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "80a5852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gk=driver.find_element(By.XPATH,\"/html/body/div/div[1]/div/div[1]/div/div[5]/div/div[1]/header/div[3]/ul/li[3]/a\")\n",
    "gk.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "ebf8c920",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div/div[2]/div/div[10]/div/div/ul/li[2]/a\")\n",
    "pm.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "0ea23e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the table with the class 'table'\n",
    "table = driver.find_elements(By.CLASS_NAME, 'table')\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "64558f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_name = []\n",
    "born_dead = []\n",
    "term = []\n",
    "remarks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "2623e463",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S.N.\\nName\\nBorn-Dead\\nTerm of office\\nRemark',\n",
       " '1.\\nJawahar Lal Nehru\\n(1889–1964)\\n15 August 1947 to 27 May 1964\\n16 years, 286 days\\nThe first prime minister of India and the longest-serving PM of India, the first to die in office.',\n",
       " '2.\\nGulzarilal Nanda (Acting)\\n(1898-1998)\\n27 May 1964 to 9 June 1964,\\n13 days\\nFirst acting PM of India',\n",
       " \"3.\\nLal Bahadur Shastri\\n(1904–1966)\\n9 June 1964 to 11 January 1966\\n1 year, 216 days\\nHe has given the slogan of 'Jai Jawan Jai Kisan' during the Indo-Pak war of 1965\",\n",
       " '4. \\nGulzari Lal Nanda  (Acting)\\n(1898-1998)\\n11 January 1966 to 24 January 1966\\n13 days\\n-',\n",
       " '5.\\nIndira Gandhi\\n(1917–1984)\\n24 January 1966 to 24 March 1977\\n11 years, 59 days\\nFirst female Prime Minister of India',\n",
       " '6.\\nMorarji Desai\\n(1896–1995)\\n24 March 1977 to  28 July 1979 \\n2 year, 126 days\\nOldest to become PM (81 years old) and first to resign from office',\n",
       " '7.\\nCharan Singh\\n(1902–1987)\\n28 July 1979 to 14 January 1980\\n170 days\\nOnly PM who did not face the Parliament',\n",
       " '8.\\nIndira Gandhi\\n(1917–1984)\\n14 January 1980 to 31 October 1984\\n4 years, 291 days\\nThe first lady who served as PM for the second term',\n",
       " '9.\\nRajiv Gandhi\\n(1944–1991)\\n31 October 1984 to 2 December 1989\\n5 years, 32 days\\nYoungest to become PM (40 years old)',\n",
       " '10.\\nV. P. Singh\\n(1931–2008)\\n2 December 1989 to 10 November 1990\\n343 days\\nFirst PM to step down after a vote of no confidence',\n",
       " '11.\\nChandra Shekhar\\n(1927–2007)\\n10 November 1990 to 21 June 1991\\n223 days\\nHe belongs to  Samajwadi Janata Party',\n",
       " '12.\\nP. V. Narasimha Rao\\n(1921–2004)\\n21 June 1991 to 16 May 1996\\n4 years, 330 days\\nFirst PM from south India',\n",
       " '13.\\nAtal Bihari Vajpayee\\n(1924- 2018)\\n16 May 1996 to 1 June 1996\\n16 days\\nPM for shortest tenure',\n",
       " '14.\\nH. D. Deve Gowda\\n(born 1933)\\n1 June 1996 to 21 April 1997\\n324 days\\nHe belongs to  Janata Dal',\n",
       " '15.\\nInder Kumar Gujral\\n(1919–2012)\\n21 April 1997 to 19 March 1998 \\n332 days\\n------',\n",
       " '16.\\nAtal Bihari Vajpayee\\n(1924-2018)\\n19 March 1998 to 22 May 2004 \\n6 years, 64 days\\n The first non-congress PM who completed a full term as PM',\n",
       " '17.\\nManmohan Singh\\n(born 1932)\\n22 May 2004 to 26 May 2014   \\n10 years, 4 days\\n First Sikh PM',\n",
       " '18.\\nNarendra Modi\\n(born 1950)\\n26 May 2014 - Present\\n4th Prime Minister of India who served two consecutive tenures',\n",
       " 'List of all Presidents of India List of Current Chief Ministers in India',\n",
       " 'List of Nicknames of Indian Prime Ministers List of Chief Ministers of Karnataka (1947-2021)']"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pm1 = driver.find_elements(By.TAG_NAME, \"tr\")\n",
    "for i in pm1:\n",
    "    pm2=i.text\n",
    "    pm_name.append(pm2)\n",
    "    \n",
    "pm_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea766abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the four lists\n",
    "names = []\n",
    "born_dead = []\n",
    "term_of_office = []\n",
    "remarks = []\n",
    "\n",
    "for item in pm_name[1:]:\n",
    "    # split each line into a list of columns\n",
    "    columns = item.split('\\n')\n",
    "    names.append(columns[1])\n",
    "    born_dead.append(columns[2])\n",
    "    term_of_office.append(columns[3])\n",
    "    remarks.append(columns[4])\n",
    "\n",
    "# print the four lists\n",
    "print(names)\n",
    "print(born_dead)\n",
    "print(term_of_office)\n",
    "print(remarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "37c71ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jawahar Lal Nehru\n",
      "(1889–1964)\n",
      "15 August 1947 to 27 May 1964\n",
      "16 years, 286 days\n",
      "\n",
      "Gulzarilal Nanda (Acting)\n",
      "(1898-1998)\n",
      "27 May 1964 to 9 June 1964,\n",
      "13 days\n",
      "\n",
      "Lal Bahadur Shastri\n",
      "(1904–1966)\n",
      "9 June 1964 to 11 January 1966\n",
      "1 year, 216 days\n",
      "\n",
      "Gulzari Lal Nanda  (Acting)\n",
      "(1898-1998)\n",
      "11 January 1966 to 24 January 1966\n",
      "13 days\n",
      "\n",
      "Indira Gandhi\n",
      "(1917–1984)\n",
      "24 January 1966 to 24 March 1977\n",
      "11 years, 59 days\n",
      "\n",
      "Morarji Desai\n",
      "(1896–1995)\n",
      "24 March 1977 to  28 July 1979 \n",
      "2 year, 126 days\n",
      "\n",
      "Charan Singh\n",
      "(1902–1987)\n",
      "28 July 1979 to 14 January 1980\n",
      "170 days\n",
      "\n",
      "Indira Gandhi\n",
      "(1917–1984)\n",
      "14 January 1980 to 31 October 1984\n",
      "4 years, 291 days\n",
      "\n",
      "Rajiv Gandhi\n",
      "(1944–1991)\n",
      "31 October 1984 to 2 December 1989\n",
      "5 years, 32 days\n",
      "\n",
      "V. P. Singh\n",
      "(1931–2008)\n",
      "2 December 1989 to 10 November 1990\n",
      "343 days\n",
      "\n",
      "Chandra Shekhar\n",
      "(1927–2007)\n",
      "10 November 1990 to 21 June 1991\n",
      "223 days\n",
      "\n",
      "P. V. Narasimha Rao\n",
      "(1921–2004)\n",
      "21 June 1991 to 16 May 1996\n",
      "4 years, 330 days\n",
      "\n",
      "Atal Bihari Vajpayee\n",
      "(1924- 2018)\n",
      "16 May 1996 to 1 June 1996\n",
      "16 days\n",
      "\n",
      "H. D. Deve Gowda\n",
      "(born 1933)\n",
      "1 June 1996 to 21 April 1997\n",
      "324 days\n",
      "\n",
      "Inder Kumar Gujral\n",
      "(1919–2012)\n",
      "21 April 1997 to 19 March 1998 \n",
      "332 days\n",
      "\n",
      "Atal Bihari Vajpayee\n",
      "(1924-2018)\n",
      "19 March 1998 to 22 May 2004 \n",
      "6 years, 64 days\n",
      "\n",
      "Manmohan Singh\n",
      "(born 1932)\n",
      "22 May 2004 to 26 May 2014   \n",
      "10 years, 4 days\n",
      "\n",
      "Narendra Modi\n",
      "(born 1950)\n",
      "26 May 2014 - Present\n",
      "4th Prime Minister of India who served two consecutive tenures\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pm_info_list = [pm.split('\\n') for pm in pm_name[1:19]]\n",
    "\n",
    "for pm_info in pm_info_list:\n",
    "    name = pm_info[1]\n",
    "    born_dead = pm_info[2]\n",
    "    term = pm_info[3]\n",
    "    remark = pm_info[4]\n",
    "    print(f\"{name}\\n{born_dead}\\n{term}\\n{remark}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3218f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df8 = pd.DataFrame(data)\n",
    "df8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dca3d82",
   "metadata": {},
   "source": [
    "# 10: Write a python program to display list of 50 Most expensive cars in the world (i.e. Car name and Price) from https://www.motor1.com/\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.motor1.com/\n",
    "2. Then You have to click on the List option from Dropdown menu on left side.\n",
    "3. Then click on 50 most expensive cars in the world..\n",
    "4. Then scrap the mentioned data and make the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54a897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\AnuragMishra\\Downloads\\chromedriver_win32.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d78172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the motor1 page on selenium automated chrome browser\n",
    "driver.get(\"https://www.motor1.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4da2689",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.XPATH,\"/html/body/div[4]/div[1]/div[3]/ul/li[6]/ul/li[1]/a\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8bca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "search1=driver.find_element(By.XPATH,\"/html/body/div[3]/div[8]/div[1]/div[1]/div/div/div[7]/div/div[1]/h3/a\")\n",
    "search1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b0711",
   "metadata": {},
   "outputs": [],
   "source": [
    "name1 = []\n",
    "price5 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bcc488",
   "metadata": {},
   "outputs": [],
   "source": [
    "car = driver.find_elements(By.XPATH, '//h3[@class=\"subheader\"]')\n",
    "for i in car:\n",
    "    names=i.text\n",
    "    name.append(names)\n",
    "    \n",
    "name1=name[51:101]\n",
    "name1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcd0d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "price3 = driver.find_elements(By.XPATH, \"//strong\")\n",
    "for i in price3:\n",
    "    price4=i.text\n",
    "    price5.append(price4)\n",
    "    \n",
    "price6 = price5[0:50]\n",
    "price6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfb530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(name1), len(price6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c99ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df9=pd.DataFrame({'Car Name':name1,'Price':price6})\n",
    "df9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caae7c65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
